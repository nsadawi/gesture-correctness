{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perkeso Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "random_seed = 42\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "#np.random.seed(seed = seed) \n",
    "#import random\n",
    "#random.seed = seed\n",
    "#import os\n",
    "#os.environ['PYTHONHASHSEED'] = '0'\n",
    "#from utils.utilities import *\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plt.style.use('ggplot')\n",
    "#from scipy import stats\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "\n",
    "import augment\n",
    "import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to know how many times we need to generate augmented data for each gesture\n",
    "def aug_times(mov):\n",
    "    a = {'0': 4, '1': 5,'2': 1, '3': 2, '4': 5, '5': 5, '6': 5, '7': 7, '8': 7}\n",
    "    return a[mov]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "patientid_date_label_repetitionNumber_correction_position.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_move_data(mov):\n",
    "    print('Loading move:', mov)\n",
    "    # patientid_date_label_repetitionNumber_correction_position.txt\n",
    "    #moves = ['0']#,'1','2','3','4','5','6','7','8']\n",
    "    labels = ['1','2']\n",
    "    subjects = ['104', '209', '205', '213', '303', '204', '201', '210', '211', '206', '103', '203', '305', '202', '217', '105',\n",
    "     '107',  '304', '302', '212', '102', '306', '208', '214', '307', '106', '216', '215', '218', '207', '301', '101']\n",
    "    #dates = ['19', '14', '12', '13', '07', '18']\n",
    "    #position = ['sit', 'stand','chair','wheelchair','Stand-frame']\n",
    "    data = pd.DataFrame()\n",
    "    #for mov in moves:\n",
    "    #print('Move: ',mov)\n",
    "    for label in labels:\n",
    "        #count = 0\n",
    "        #print('Label: ',label)\n",
    "        for subj in subjects:\n",
    "            #print('Subj: ',subj)\n",
    "            for file_name in glob.glob('/home/noureddin/Perkeso-Simplified/'+subj+'_*_'+mov+'_*_'+label+'_*.txt'):\n",
    "                #count += 1\n",
    "                tmp_df = pd.read_csv(file_name,header=None)\n",
    "                if len(tmp_df) >= 28:\n",
    "                    tmp_ = pd.DataFrame()#for augmentation\n",
    "                    if label == '2':\n",
    "                        #now augment data                        \n",
    "                        augment_times = aug_times(mov)\n",
    "                        for i in range(0,augment_times):                \n",
    "                            tdata = tmp_df.values\n",
    "                            #dd = DA_TimeWarp(DA_Permutation(DA_Rotation(data)))\n",
    "                            dd = augment.DA_TimeWarp3(augment.DA_Rotation((tdata)))\n",
    "                            #make sure the # of rows is 28\n",
    "                            dd = preprocess.normalize_move(pd.DataFrame(dd), 28, mean=False)\n",
    "                            #transform values into specific range\n",
    "                            dd = preprocess.normalize_df(dd) # e.g. all cols in range [0,1]\n",
    "                            #mean centre the df\n",
    "                            dd = preprocess.mean_centre_data(dd)                \n",
    "                            tmp_ = tmp_.append(dd,ignore_index=True)\n",
    "                \n",
    "                    #make sure the # of rows is 28\n",
    "                    tmp_df = preprocess.normalize_move(tmp_df, 28, mean=False)\n",
    "                    #transform values into specific range\n",
    "                    tmp_df = preprocess.normalize_df(tmp_df) # e.g. all cols in range [0,1] \n",
    "                    #mean centre the df\n",
    "                    tmp_df = preprocess.mean_centre_data(tmp_df)\n",
    "                    tmp_df['Original'] = 1 \n",
    "                    if len(tmp_) > 0:\n",
    "                        tmp_['Original']   = 0\n",
    "                        tmp_df = tmp_df.append(tmp_,ignore_index=True)\n",
    "                \n",
    "                    tmp_df.columns = ['f_'+str(i) for i in range(tmp_df.shape[1])]\n",
    "                    tmp_df.rename(columns={'f_75':'Original'}, inplace=True)\n",
    "                    \n",
    "                    tmp_df['Subject'] = subj\n",
    "                    tmp_df['Class'] = 1 if label == '1' else 0\n",
    "                    data = data.append(tmp_df,ignore_index=True)\n",
    "                    #print(data.shape)   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = load_move_data('6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data[data['Class'] == 1].shape)\n",
    "#print(data[data['Class'] == 0].shape)\n",
    "#data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "#import tensorflow as tf\n",
    "\n",
    "input_width = 28\n",
    "seq_len = input_width          # Number of steps\n",
    "\n",
    "lstm_size = 225     # 3 times the amount of channels\n",
    "lstm_layers = 3        # Number of layers\n",
    "batch_size = 2       # Batch size\n",
    "\n",
    "learning_rate = 0.0001  # Learning rate (default is 0.001)\n",
    "epochs = 50\n",
    "\n",
    "# Fixed\n",
    "n_classes = 2\n",
    "\n",
    "num_features = 75\n",
    "n_channels = num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network\n",
    "* Placeholders\n",
    "* Build Convolutional Layers (Note: Should we use a different activation? Like tf.nn.tanh?)\n",
    "* Then flatten and pass to the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading move: 3\n",
      "Doing subj: 104\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 104 took:  0:04:18.710293\n",
      "Doing subj: 209\n",
      "(5, 28, 75) (5, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 209 took:  0:04:20.224329\n",
      "Doing subj: 205\n",
      "Doing subj: 213\n",
      "(7, 28, 75) (7, 2)\n",
      "Test accuracy: 0.166667\n",
      "Subj: 213 took:  0:04:11.790668\n",
      "Doing subj: 303\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 303 took:  0:04:16.345867\n",
      "Doing subj: 204\n",
      "(58, 28, 75) (58, 2)\n",
      "Test accuracy: 0.879310\n",
      "Subj: 204 took:  0:03:12.666906\n",
      "Doing subj: 201\n",
      "(17, 28, 75) (17, 2)\n",
      "Test accuracy: 0.875000\n",
      "Subj: 201 took:  0:03:58.859364\n",
      "Doing subj: 210\n",
      "(5, 28, 75) (5, 2)\n",
      "Test accuracy: 0.750000\n",
      "Subj: 210 took:  0:04:20.141209\n",
      "Doing subj: 211\n",
      "(5, 28, 75) (5, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 211 took:  0:04:19.916880\n",
      "Doing subj: 206\n",
      "(17, 28, 75) (17, 2)\n",
      "Test accuracy: 0.875000\n",
      "Subj: 206 took:  0:03:57.605159\n",
      "Doing subj: 103\n",
      "(24, 28, 75) (24, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 103 took:  0:04:10.792570\n",
      "Doing subj: 203\n",
      "(4, 28, 75) (4, 2)\n",
      "Test accuracy: 0.500000\n",
      "Subj: 203 took:  0:04:19.676549\n",
      "Doing subj: 305\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 305 took:  0:04:18.738119\n",
      "Doing subj: 202\n",
      "Doing subj: 217\n",
      "(7, 28, 75) (7, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 217 took:  0:04:18.043185\n",
      "Doing subj: 105\n",
      "(14, 28, 75) (14, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 105 took:  0:04:15.017002\n",
      "Doing subj: 107\n",
      "Doing subj: 304\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 304 took:  0:04:19.590575\n",
      "Doing subj: 302\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 302 took:  0:04:16.398061\n",
      "Doing subj: 212\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.833333\n",
      "Subj: 212 took:  0:04:17.500853\n",
      "Doing subj: 102\n",
      "(24, 28, 75) (24, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 102 took:  0:04:10.199556\n",
      "Doing subj: 306\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 306 took:  0:04:16.120487\n",
      "Doing subj: 208\n",
      "Doing subj: 214\n",
      "(5, 28, 75) (5, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 214 took:  0:04:20.124162\n",
      "Doing subj: 307\n",
      "(11, 28, 75) (11, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 307 took:  0:04:17.187080\n",
      "Doing subj: 106\n",
      "Doing subj: 216\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 216 took:  0:04:18.946749\n",
      "Doing subj: 215\n",
      "(7, 28, 75) (7, 2)\n",
      "Test accuracy: 0.833333\n",
      "Subj: 215 took:  0:04:16.917388\n",
      "Doing subj: 218\n",
      "Doing subj: 207\n",
      "(16, 28, 75) (16, 2)\n",
      "Test accuracy: 0.750000\n",
      "Subj: 207 took:  0:03:59.077830\n",
      "Doing subj: 301\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 301 took:  0:04:17.281591\n",
      "Doing subj: 101\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 0.833333\n",
      "Subj: 101 took:  0:04:15.968574\n",
      "====================================\n",
      "Loading move: 4\n",
      "Doing subj: 104\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.666667\n",
      "Subj: 104 took:  0:04:00.042353\n",
      "Doing subj: 209\n",
      "Doing subj: 205\n",
      "Doing subj: 213\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 213 took:  0:03:59.542525\n",
      "Doing subj: 303\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 0.750000\n",
      "Subj: 303 took:  0:03:56.967879\n",
      "Doing subj: 204\n",
      "(7, 28, 75) (7, 2)\n",
      "Test accuracy: 0.500000\n",
      "Subj: 204 took:  0:03:50.406639\n",
      "Doing subj: 201\n",
      "(7, 28, 75) (7, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 201 took:  0:03:43.186549\n",
      "Doing subj: 210\n",
      "(5, 28, 75) (5, 2)\n",
      "Test accuracy: 0.750000\n",
      "Subj: 210 took:  0:04:00.319145\n",
      "Doing subj: 211\n",
      "(5, 28, 75) (5, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 211 took:  0:03:48.282068\n",
      "Doing subj: 206\n",
      "(13, 28, 75) (13, 2)\n",
      "Test accuracy: 0.416667\n",
      "Subj: 206 took:  0:03:46.969749\n",
      "Doing subj: 103\n",
      "(25, 28, 75) (25, 2)\n",
      "Test accuracy: 0.875000\n",
      "Subj: 103 took:  0:03:48.328156\n",
      "Doing subj: 203\n",
      "Doing subj: 305\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 0.833333\n",
      "Subj: 305 took:  0:03:57.759322\n",
      "Doing subj: 202\n",
      "(36, 28, 75) (36, 2)\n",
      "Test accuracy: 0.416667\n",
      "Subj: 202 took:  0:03:23.322487\n",
      "Doing subj: 217\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 217 took:  0:04:00.227242\n",
      "Doing subj: 105\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 105 took:  0:03:57.929083\n",
      "Doing subj: 107\n",
      "Doing subj: 304\n",
      "(7, 28, 75) (7, 2)\n",
      "Test accuracy: 0.833333\n",
      "Subj: 304 took:  0:03:59.162814\n",
      "Doing subj: 302\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 302 took:  0:03:57.406553\n",
      "Doing subj: 212\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.666667\n",
      "Subj: 212 took:  0:04:00.287836\n",
      "Doing subj: 102\n",
      "(25, 28, 75) (25, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 102 took:  0:03:49.245620\n",
      "Doing subj: 306\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 306 took:  0:03:57.642051\n",
      "Doing subj: 208\n",
      "Doing subj: 214\n",
      "(5, 28, 75) (5, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 214 took:  0:04:00.120827\n",
      "Doing subj: 307\n",
      "(13, 28, 75) (13, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 307 took:  0:03:56.617630\n",
      "Doing subj: 106\n",
      "Doing subj: 216\n",
      "(7, 28, 75) (7, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 216 took:  0:03:42.447285\n",
      "Doing subj: 215\n",
      "(10, 28, 75) (10, 2)\n",
      "Test accuracy: 0.600000\n",
      "Subj: 215 took:  0:03:48.382476\n",
      "Doing subj: 218\n",
      "Doing subj: 207\n",
      "Doing subj: 301\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 301 took:  0:03:56.893203\n",
      "Doing subj: 101\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 0.833333\n",
      "Subj: 101 took:  0:03:54.342342\n",
      "====================================\n",
      "Loading move: 5\n",
      "Doing subj: 104\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 104 took:  0:03:39.186981\n",
      "Doing subj: 209\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 209 took:  0:03:39.873626\n",
      "Doing subj: 205\n",
      "Doing subj: 213\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.500000\n",
      "Subj: 213 took:  0:03:38.209892\n",
      "Doing subj: 303\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 0.916667\n",
      "Subj: 303 took:  0:03:37.151272\n",
      "Doing subj: 204\n",
      "(7, 28, 75) (7, 2)\n",
      "Test accuracy: 0.666667\n",
      "Subj: 204 took:  0:03:32.208152\n",
      "Doing subj: 201\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 201 took:  0:03:25.026652\n",
      "Doing subj: 210\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.666667\n",
      "Subj: 210 took:  0:03:37.768202\n",
      "Doing subj: 211\n",
      "(5, 28, 75) (5, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 211 took:  0:03:40.613873\n",
      "Doing subj: 206\n",
      "(20, 28, 75) (20, 2)\n",
      "Test accuracy: 0.500000\n",
      "Subj: 206 took:  0:02:44.170473\n",
      "Doing subj: 103\n",
      "(24, 28, 75) (24, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 103 took:  0:03:31.507254\n",
      "Doing subj: 203\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 0.666667\n",
      "Subj: 203 took:  0:03:27.057482\n",
      "Doing subj: 305\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 0.916667\n",
      "Subj: 305 took:  0:03:36.928549\n",
      "Doing subj: 202\n",
      "Doing subj: 217\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 217 took:  0:03:39.969874\n",
      "Doing subj: 105\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 0.833333\n",
      "Subj: 105 took:  0:03:37.024461\n",
      "Doing subj: 107\n",
      "Doing subj: 304\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 304 took:  0:03:39.595220\n",
      "Doing subj: 302\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 302 took:  0:03:37.761223\n",
      "Doing subj: 212\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.666667\n",
      "Subj: 212 took:  0:03:35.681504\n",
      "Doing subj: 102\n",
      "(24, 28, 75) (24, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 102 took:  0:03:31.246512\n",
      "Doing subj: 306\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 306 took:  0:03:36.805269\n",
      "Doing subj: 208\n",
      "Doing subj: 214\n",
      "(5, 28, 75) (5, 2)\n",
      "Test accuracy: 0.750000\n",
      "Subj: 214 took:  0:03:41.095024\n",
      "Doing subj: 307\n",
      "(11, 28, 75) (11, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 307 took:  0:03:39.096391\n",
      "Doing subj: 106\n",
      "Doing subj: 216\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 216 took:  0:03:40.357373\n",
      "Doing subj: 215\n",
      "(9, 28, 75) (9, 2)\n",
      "Test accuracy: 0.875000\n",
      "Subj: 215 took:  0:03:39.306090\n",
      "Doing subj: 218\n",
      "Doing subj: 207\n",
      "Doing subj: 301\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 0.916667\n",
      "Subj: 301 took:  0:03:37.039488\n",
      "Doing subj: 101\n",
      "(13, 28, 75) (13, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 101 took:  0:03:34.935945\n",
      "====================================\n",
      "Loading move: 6\n",
      "Doing subj: 104\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 104 took:  0:03:47.815452\n",
      "Doing subj: 209\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 209 took:  0:03:48.350098\n",
      "Doing subj: 205\n",
      "Doing subj: 213\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 213 took:  0:03:48.104208\n",
      "Doing subj: 303\n",
      "(11, 28, 75) (11, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 303 took:  0:03:46.931768\n",
      "Doing subj: 204\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.833333\n",
      "Subj: 204 took:  0:03:48.063905\n",
      "Doing subj: 201\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.666667\n",
      "Subj: 201 took:  0:03:33.741188\n",
      "Doing subj: 210\n",
      "(5, 28, 75) (5, 2)\n",
      "Test accuracy: 0.000000\n",
      "Subj: 210 took:  0:03:37.028179\n",
      "Doing subj: 211\n",
      "(5, 28, 75) (5, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 211 took:  0:03:49.234075\n",
      "Doing subj: 206\n",
      "(14, 28, 75) (14, 2)\n",
      "Test accuracy: 0.857143\n",
      "Subj: 206 took:  0:03:32.425544\n",
      "Doing subj: 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 28, 75) (24, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 103 took:  0:03:40.099122\n",
      "Doing subj: 203\n",
      "Doing subj: 305\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 305 took:  0:03:45.458702\n",
      "Doing subj: 202\n",
      "Doing subj: 217\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.833333\n",
      "Subj: 217 took:  0:03:48.173097\n",
      "Doing subj: 105\n",
      "(16, 28, 75) (16, 2)\n",
      "Test accuracy: 0.750000\n",
      "Subj: 105 took:  0:03:43.556393\n",
      "Doing subj: 107\n",
      "Doing subj: 304\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 304 took:  0:03:48.204560\n",
      "Doing subj: 302\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 302 took:  0:03:45.299959\n",
      "Doing subj: 212\n",
      "(5, 28, 75) (5, 2)\n",
      "Test accuracy: 0.750000\n",
      "Subj: 212 took:  0:03:39.235740\n",
      "Doing subj: 102\n",
      "(24, 28, 75) (24, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 102 took:  0:03:39.699397\n",
      "Doing subj: 306\n",
      "(13, 28, 75) (13, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 306 took:  0:03:45.542463\n",
      "Doing subj: 208\n",
      "Doing subj: 214\n",
      "(4, 28, 75) (4, 2)\n",
      "Test accuracy: 0.500000\n",
      "Subj: 214 took:  0:03:49.139737\n",
      "Doing subj: 307\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 0.916667\n",
      "Subj: 307 took:  0:03:45.208973\n",
      "Doing subj: 106\n",
      "Doing subj: 216\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.833333\n",
      "Subj: 216 took:  0:03:48.134659\n",
      "Doing subj: 215\n",
      "(9, 28, 75) (9, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 215 took:  0:03:44.203121\n",
      "Doing subj: 218\n",
      "Doing subj: 207\n",
      "(20, 28, 75) (20, 2)\n",
      "Test accuracy: 0.950000\n",
      "Subj: 207 took:  0:02:52.468638\n",
      "Doing subj: 301\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 0.916667\n",
      "Subj: 301 took:  0:03:45.289156\n",
      "Doing subj: 101\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 101 took:  0:03:45.091890\n",
      "====================================\n",
      "Loading move: 7\n",
      "Doing subj: 104\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.833333\n",
      "Subj: 104 took:  0:04:08.234646\n",
      "Doing subj: 209\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.333333\n",
      "Subj: 209 took:  0:03:47.506401\n",
      "Doing subj: 205\n",
      "Doing subj: 213\n",
      "(5, 28, 75) (5, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 213 took:  0:04:08.538777\n",
      "Doing subj: 303\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 0.916667\n",
      "Subj: 303 took:  0:04:05.267485\n",
      "Doing subj: 204\n",
      "(7, 28, 75) (7, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 204 took:  0:04:03.854418\n",
      "Doing subj: 201\n",
      "Doing subj: 210\n",
      "Doing subj: 211\n",
      "(7, 28, 75) (7, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 211 took:  0:03:52.389250\n",
      "Doing subj: 206\n",
      "(25, 28, 75) (25, 2)\n",
      "Test accuracy: 0.750000\n",
      "Subj: 206 took:  0:03:42.190757\n",
      "Doing subj: 103\n",
      "(24, 28, 75) (24, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 103 took:  0:03:59.413731\n",
      "Doing subj: 203\n",
      "Doing subj: 305\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 305 took:  0:04:05.013326\n",
      "Doing subj: 202\n",
      "(23, 28, 75) (23, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 202 took:  0:03:49.569294\n",
      "Doing subj: 217\n",
      "(7, 28, 75) (7, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 217 took:  0:04:03.445158\n",
      "Doing subj: 105\n",
      "(13, 28, 75) (13, 2)\n",
      "Test accuracy: 0.916667\n",
      "Subj: 105 took:  0:04:03.927300\n",
      "Doing subj: 107\n",
      "Doing subj: 304\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.833333\n",
      "Subj: 304 took:  0:04:08.525729\n",
      "Doing subj: 302\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 0.833333\n",
      "Subj: 302 took:  0:04:05.851358\n",
      "Doing subj: 212\n",
      "(5, 28, 75) (5, 2)\n",
      "Test accuracy: 0.500000\n",
      "Subj: 212 took:  0:03:52.468390\n",
      "Doing subj: 102\n",
      "(24, 28, 75) (24, 2)\n",
      "Test accuracy: 0.958333\n",
      "Subj: 102 took:  0:03:59.748681\n",
      "Doing subj: 306\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 0.916667\n",
      "Subj: 306 took:  0:04:05.174119\n",
      "Doing subj: 208\n",
      "Doing subj: 214\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.833333\n",
      "Subj: 214 took:  0:04:08.363196\n",
      "Doing subj: 307\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 307 took:  0:04:05.020668\n",
      "Doing subj: 106\n",
      "Doing subj: 216\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.500000\n",
      "Subj: 216 took:  0:04:08.198736\n",
      "Doing subj: 215\n",
      "(8, 28, 75) (8, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 215 took:  0:04:06.720165\n",
      "Doing subj: 218\n",
      "Doing subj: 207\n",
      "(9, 28, 75) (9, 2)\n",
      "Test accuracy: 0.500000\n",
      "Subj: 207 took:  0:03:39.109642\n",
      "Doing subj: 301\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 301 took:  0:04:05.362826\n",
      "Doing subj: 101\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 101 took:  0:04:05.390696\n",
      "====================================\n",
      "Loading move: 8\n",
      "Doing subj: 104\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 104 took:  0:04:04.088366\n",
      "Doing subj: 209\n",
      "(4, 28, 75) (4, 2)\n",
      "Test accuracy: 0.750000\n",
      "Subj: 209 took:  0:04:05.385786\n",
      "Doing subj: 205\n",
      "Doing subj: 213\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.333333\n",
      "Subj: 213 took:  0:03:49.845008\n",
      "Doing subj: 303\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 0.916667\n",
      "Subj: 303 took:  0:04:01.783515\n",
      "Doing subj: 204\n",
      "(13, 28, 75) (13, 2)\n",
      "Test accuracy: 0.916667\n",
      "Subj: 204 took:  0:03:39.966442\n",
      "Doing subj: 201\n",
      "Doing subj: 210\n",
      "Doing subj: 211\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 211 took:  0:04:03.990188\n",
      "Doing subj: 206\n",
      "(10, 28, 75) (10, 2)\n",
      "Test accuracy: 0.600000\n",
      "Subj: 206 took:  0:03:51.619282\n",
      "Doing subj: 103\n",
      "(25, 28, 75) (25, 2)\n",
      "Test accuracy: 0.958333\n",
      "Subj: 103 took:  0:03:55.524884\n",
      "Doing subj: 203\n",
      "Doing subj: 305\n",
      "(11, 28, 75) (11, 2)\n",
      "Test accuracy: 0.900000\n",
      "Subj: 305 took:  0:04:01.710838\n",
      "Doing subj: 202\n",
      "(47, 28, 75) (47, 2)\n",
      "Test accuracy: 0.869565\n",
      "Subj: 202 took:  0:03:34.237713\n",
      "Doing subj: 217\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 217 took:  0:04:04.908926\n",
      "Doing subj: 105\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 105 took:  0:04:01.464299\n",
      "Doing subj: 107\n",
      "Doing subj: 304\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.833333\n",
      "Subj: 304 took:  0:04:04.065540\n",
      "Doing subj: 302\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 302 took:  0:04:01.128371\n",
      "Doing subj: 212\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 0.666667\n",
      "Subj: 212 took:  0:03:43.798861\n",
      "Doing subj: 102\n",
      "(24, 28, 75) (24, 2)\n",
      "Test accuracy: 0.958333\n",
      "Subj: 102 took:  0:03:56.359063\n",
      "Doing subj: 306\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 306 took:  0:04:01.541718\n",
      "Doing subj: 208\n",
      "Doing subj: 214\n",
      "(7, 28, 75) (7, 2)\n",
      "Test accuracy: 0.333333\n",
      "Subj: 214 took:  0:03:56.154434\n",
      "Doing subj: 307\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 0.916667\n",
      "Subj: 307 took:  0:04:01.549233\n",
      "Doing subj: 106\n",
      "Doing subj: 216\n",
      "(6, 28, 75) (6, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 216 took:  0:04:04.268500\n",
      "Doing subj: 215\n",
      "(10, 28, 75) (10, 2)\n",
      "Test accuracy: 0.800000\n",
      "Subj: 215 took:  0:04:02.319506\n",
      "Doing subj: 218\n",
      "Doing subj: 207\n",
      "(7, 28, 75) (7, 2)\n",
      "Test accuracy: 0.166667\n",
      "Subj: 207 took:  0:03:42.472786\n",
      "Doing subj: 301\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 301 took:  0:04:01.189015\n",
      "Doing subj: 101\n",
      "(12, 28, 75) (12, 2)\n",
      "Test accuracy: 1.000000\n",
      "Subj: 101 took:  0:04:01.202480\n",
      "====================================\n",
      "Overall took:  9:40:45.725464\n"
     ]
    }
   ],
   "source": [
    "moves = ['0','1','2', '3','4','5','6','7','8']\n",
    "overall_results = {}\n",
    "start_time = datetime.now()\n",
    "for mov in moves:\n",
    "    appended_data = load_move_data(mov)\n",
    "    #subjects = get_subjects(mov)\n",
    "    subj_results = {}\n",
    "    \n",
    "    for test_subj in subjects:\n",
    "        print('Doing subj:', test_subj)\n",
    "        subj_start_time = datetime.now()\n",
    "        ### Split data into Train and Test\n",
    "        #test_subj           = 's06'\n",
    "        test_subject        = appended_data[appended_data['Subject'] == test_subj]\n",
    "        # test only on original data .. don't include augment data\n",
    "        test_subject = test_subject[test_subject['Original'] == 1]\n",
    "        \n",
    "        train_subjects         = appended_data[appended_data['Subject'] != test_subj]\n",
    "        \n",
    "        X_train, labels_train  = preprocess.segment_signal(train_subjects, input_width, num_features)\n",
    "        X_test, labels_test    = preprocess.segment_signal(test_subject, input_width, num_features)\n",
    "        \n",
    "        # Normalize?\n",
    "        #X_train, mean_   = zero_mean(X_train,0) \n",
    "        ##X_train, mean_, std_    = standardize(X_train,0,0)\n",
    "        # replace NaNs with 0's\n",
    "        X_train[np.isnan(X_train)] = 0\n",
    "\n",
    "        # Normalize?\n",
    "        #X_test, _        = zero_mean(X_test,mean_, train=False)\n",
    "        ##X_test, _, _            = standardize(X_test,mean_, std_, train=False)\n",
    "        # replace NaNs with 0's\n",
    "        X_test[np.isnan(X_test)] = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        #X_train, mean, std    = normalize(X_train)\n",
    "        #X_test                = normalize(X_test)\n",
    "\n",
    "        #### One-hot encoding for labels:\n",
    "        # Notice that we add two extra elements 0,1\n",
    "        # Because sometimes we only have correct or incorrect\n",
    "        # move for some subjects and this makes one hot encoding not possible\n",
    "        # one_hot and then drop the elements we added    \n",
    "        y_tr   = preprocess.one_hot(np.append(labels_train, [0,1]))[:-2]\n",
    "        y_test = preprocess.one_hot(np.append(labels_test, [0,1]))[:-2]\n",
    "        #we make sure there's at least 2 test episodes for this test subject\n",
    "        #otherwise no point training the LSTM\n",
    "        if len(y_test) > 1:\n",
    "            ### Construct the graph\n",
    "            tf.reset_default_graph()\n",
    "            graph = tf.Graph()\n",
    "            # Construct placeholders\n",
    "            with graph.as_default(), tf.device('/gpu:0'):\n",
    "                #Placeholders\n",
    "                inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')\n",
    "                labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels')\n",
    "                keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "                learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "                # Construct the LSTM inputs and LSTM cells\n",
    "                lstm_in = tf.transpose(inputs_, [1,0,2]) # reshape into (seq_len, N, channels)\n",
    "                lstm_in = tf.reshape(lstm_in, [-1, n_channels]) # Now (seq_len*N, n_channels)\n",
    "\n",
    "                # To cells\n",
    "                lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=None) # or tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh?\n",
    "\n",
    "                # Open up the tensor into a list of seq_len pieces\n",
    "                lstm_in = tf.split(lstm_in, seq_len, 0)\n",
    "\n",
    "                # Add LSTM layers\n",
    "                lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "                drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n",
    "                cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "                initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "                ## Define forward pass, cost function and optimizer:\n",
    "                outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32,\n",
    "                                                         initial_state = initial_state)\n",
    "\n",
    "                # We only need the last output tensor to pass into a classifier\n",
    "                logits = tf.layers.dense(outputs[-1], n_classes, name='logits')\n",
    "\n",
    "                # Cost function and optimizer\n",
    "                cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n",
    "                #optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost) # No grad clipping\n",
    "\n",
    "                # Grad clipping\n",
    "                train_op = tf.train.AdamOptimizer(learning_rate_)\n",
    "\n",
    "                gradients = train_op.compute_gradients(cost)\n",
    "                capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "                optimizer = train_op.apply_gradients(capped_gradients)\n",
    "\n",
    "                # Accuracy\n",
    "                correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "            with tf.Session(graph=graph) as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                #iteration = 1\n",
    "\n",
    "                for e in range(epochs):\n",
    "                    # Initialize \n",
    "                    state = sess.run(initial_state)\n",
    "                    #print(\"Epoch: {}/{}\".format(e, epochs))\n",
    "                    #iteration = 1\n",
    "                    # Loop over batches\n",
    "                    #for x,y in get_random_batches(X_tr, y_tr, num_batches,batch_size):\n",
    "                    #for x,y in get_batches(X_tr, y_tr, batch_size):\n",
    "                    for x,y in preprocess.get_batches(X_train, y_tr, batch_size, shuffle=True):\n",
    "\n",
    "                        # Feed dictionary\n",
    "                        feed = {inputs_ : x, labels_ : y, keep_prob_ : 0.5, \n",
    "                                initial_state : state, learning_rate_ : learning_rate}\n",
    "\n",
    "                        loss, _ , state, acc = sess.run([cost, optimizer, final_state, accuracy], \n",
    "                                                         feed_dict = feed)\n",
    "\n",
    "                test_acc = []\n",
    "\n",
    "\n",
    "                # Restore\n",
    "                #saver.restore(sess, tf.train.latest_checkpoint(save_dir))\n",
    "                test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                print(X_test.shape, y_test.shape)\n",
    "                for x_t, y_t in preprocess.get_batches(X_test, y_test, batch_size, shuffle=False):\n",
    "                    feed = {inputs_: x_t,\n",
    "                            labels_: y_t,\n",
    "                            keep_prob_: 1,\n",
    "                            initial_state: test_state}\n",
    "\n",
    "                    batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    test_acc.append(batch_acc)\n",
    "                print(\"Test accuracy: {:.6f}\".format(np.mean(test_acc)))\n",
    "                subj_results[test_subj] = np.mean(test_acc)\n",
    "            print(\"Subj:\", test_subj,\"took: \", datetime.now() - subj_start_time)\n",
    "            #shutil.rmtree(save_dir, ignore_errors=True)\n",
    "    overall_results[mov] = subj_results\n",
    "    print(\"====================================\")\n",
    "print(\"Overall took: \", datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results in JSON fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('NIPS19-Perkeso-LSTM-Results.json', 'w') as outfile:\n",
    "    json.dump(str(overall_results), outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('NIPS19-Perkeso-LSTM-Results.json') as json_file:  \n",
    "    overall_results = json.loads(json_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_results = {'0': {'104': 1.0, '209': 0.8333333, '205': 1.0, '213': 1.0, '303': 1.0, '204': 1.0, '201': 0.6666667, '211': 0.75, '206': 1.0, '103': 0.9583333, '305': 1.0, '217': 0.75, '105': 1.0, '304': 1.0, '302': 1.0, '212': 0.6666667, '102': 1.0, '306': 0.75, '214': 0.6666667, '307': 0.9, '216': 0.8333333, '215': 0.6666667, '207': 1.0, '301': 1.0, '101': 0.875}, '1': {'104': 1.0, '209': 0.6666667, '205': 0.75, '213': 0.5, '303': 1.0, '204': 0.8333333, '201': 0.85714287, '210': 0.8333333, '211': 1.0, '206': 0.75, '103': 1.0, '305': 0.9, '217': 0.8333333, '105': 1.0, '304': 0.33333334, '302': 0.9166667, '212': 0.8333333, '102': 1.0, '306': 0.9166667, '214': 0.5, '307': 0.9166667, '216': 0.33333334, '215': 0.0, '207': 0.5714286, '301': 1.0, '101': 1.0}, '2': {'104': 1.0, '209': 0.41666666, '213': 1.0, '303': 1.0, '204': 0.7586207, '201': 0.9130435, '210': 0.0, '211': 1.0, '206': 0.875, '103': 1.0, '203': 0.85714287, '305': 1.0, '217': 1.0, '105': 1.0, '304': 1.0, '302': 0.9166667, '212': 0.8333333, '102': 0.9583333, '306': 1.0, '214': 0.6666667, '307': 0.9166667, '216': 0.75, '215': 0.8, '301': 0.9166667, '101': 1.0}, '3': {'104': 1.0, '209': 1.0, '213': 0.16666667, '303': 1.0, '204': 0.87931037, '201': 0.875, '210': 0.75, '211': 1.0, '206': 0.875, '103': 1.0, '203': 0.5, '305': 1.0, '217': 1.0, '105': 1.0, '304': 1.0, '302': 1.0, '212': 0.8333333, '102': 1.0, '306': 1.0, '214': 1.0, '307': 1.0, '216': 1.0, '215': 0.8333333, '207': 0.75, '301': 1.0, '101': 0.8333333}, '4': {'104': 0.6666667, '213': 1.0, '303': 0.75, '204': 0.5, '201': 1.0, '210': 0.75, '211': 1.0, '206': 0.41666666, '103': 0.875, '305': 0.8333333, '202': 0.41666666, '217': 1.0, '105': 1.0, '304': 0.8333333, '302': 1.0, '212': 0.6666667, '102': 1.0, '306': 1.0, '214': 1.0, '307': 1.0, '216': 1.0, '215': 0.6, '301': 1.0, '101': 0.8333333}, '5': {'104': 1.0, '209': 1.0, '213': 0.5, '303': 0.9166667, '204': 0.6666667, '201': 1.0, '210': 0.6666667, '211': 1.0, '206': 0.5, '103': 1.0, '203': 0.6666667, '305': 0.9166667, '217': 1.0, '105': 0.8333333, '304': 1.0, '302': 1.0, '212': 0.6666667, '102': 1.0, '306': 1.0, '214': 0.75, '307': 1.0, '216': 1.0, '215': 0.875, '301': 0.9166667, '101': 1.0}, '6': {'104': 1.0, '209': 1.0, '213': 1.0, '303': 1.0, '204': 0.8333333, '201': 0.6666667, '210': 0.0, '211': 1.0, '206': 0.85714287, '103': 1.0, '305': 1.0, '217': 0.8333333, '105': 0.75, '304': 1.0, '302': 1.0, '212': 0.75, '102': 1.0, '306': 1.0, '214': 0.5, '307': 0.9166667, '216': 0.8333333, '215': 1.0, '207': 0.95, '301': 0.9166667, '101': 1.0}, '7': {'104': 0.8333333, '209': 0.33333334, '213': 1.0, '303': 0.9166667, '204': 1.0, '211': 1.0, '206': 0.75, '103': 1.0, '305': 1.0, '202': 1.0, '217': 1.0, '105': 0.9166667, '304': 0.8333333, '302': 0.8333333, '212': 0.5, '102': 0.9583333, '306': 0.9166667, '214': 0.8333333, '307': 1.0, '216': 0.5, '215': 1.0, '207': 0.5, '301': 1.0, '101': 1.0}, '8': {'104': 1.0, '209': 0.75, '213': 0.33333334, '303': 0.9166667, '204': 0.9166667, '211': 1.0, '206': 0.6, '103': 0.9583333, '305': 0.9, '202': 0.8695652, '217': 1.0, '105': 1.0, '304': 0.8333333, '302': 1.0, '212': 0.6666667, '102': 0.9583333, '306': 1.0, '214': 0.33333334, '307': 0.9166667, '216': 1.0, '215': 0.8, '207': 0.16666667, '301': 1.0, '101': 1.0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8926666679999999\n",
      "1 0.7786630057692308\n",
      "2 0.8631522852000001\n",
      "3 0.895999113076923\n",
      "4 0.8392361091666668\n",
      "5 0.8750000080000001\n",
      "6 0.8722857148\n",
      "7 0.8593749974999999\n",
      "8 0.8299818854166667\n"
     ]
    }
   ],
   "source": [
    "for k,v in overall_results.items():\n",
    "    vals = (list(v.values()))\n",
    "    avg = sum(vals) / len(vals)\n",
    "    print(k,avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'104': 1.0,\n",
       "  '209': 0.8333333,\n",
       "  '205': 1.0,\n",
       "  '213': 1.0,\n",
       "  '303': 1.0,\n",
       "  '204': 1.0,\n",
       "  '201': 0.6666667,\n",
       "  '211': 0.75,\n",
       "  '206': 1.0,\n",
       "  '103': 0.9583333,\n",
       "  '305': 1.0,\n",
       "  '217': 0.75,\n",
       "  '105': 1.0,\n",
       "  '304': 1.0,\n",
       "  '302': 1.0,\n",
       "  '212': 0.6666667,\n",
       "  '102': 1.0,\n",
       "  '306': 0.75,\n",
       "  '214': 0.6666667,\n",
       "  '307': 0.9,\n",
       "  '216': 0.8333333,\n",
       "  '215': 0.6666667,\n",
       "  '207': 1.0,\n",
       "  '301': 1.0,\n",
       "  '101': 0.875},\n",
       " '1': {'104': 1.0,\n",
       "  '209': 0.6666667,\n",
       "  '205': 0.75,\n",
       "  '213': 0.5,\n",
       "  '303': 1.0,\n",
       "  '204': 0.8333333,\n",
       "  '201': 0.85714287,\n",
       "  '210': 0.8333333,\n",
       "  '211': 1.0,\n",
       "  '206': 0.75,\n",
       "  '103': 1.0,\n",
       "  '305': 0.9,\n",
       "  '217': 0.8333333,\n",
       "  '105': 1.0,\n",
       "  '304': 0.33333334,\n",
       "  '302': 0.9166667,\n",
       "  '212': 0.8333333,\n",
       "  '102': 1.0,\n",
       "  '306': 0.9166667,\n",
       "  '214': 0.5,\n",
       "  '307': 0.9166667,\n",
       "  '216': 0.33333334,\n",
       "  '215': 0.0,\n",
       "  '207': 0.5714286,\n",
       "  '301': 1.0,\n",
       "  '101': 1.0},\n",
       " '2': {'104': 1.0,\n",
       "  '209': 0.41666666,\n",
       "  '213': 1.0,\n",
       "  '303': 1.0,\n",
       "  '204': 0.7586207,\n",
       "  '201': 0.9130435,\n",
       "  '210': 0.0,\n",
       "  '211': 1.0,\n",
       "  '206': 0.875,\n",
       "  '103': 1.0,\n",
       "  '203': 0.85714287,\n",
       "  '305': 1.0,\n",
       "  '217': 1.0,\n",
       "  '105': 1.0,\n",
       "  '304': 1.0,\n",
       "  '302': 0.9166667,\n",
       "  '212': 0.8333333,\n",
       "  '102': 0.9583333,\n",
       "  '306': 1.0,\n",
       "  '214': 0.6666667,\n",
       "  '307': 0.9166667,\n",
       "  '216': 0.75,\n",
       "  '215': 0.8,\n",
       "  '301': 0.9166667,\n",
       "  '101': 1.0},\n",
       " '3': {'104': 1.0,\n",
       "  '209': 1.0,\n",
       "  '213': 0.16666667,\n",
       "  '303': 1.0,\n",
       "  '204': 0.87931037,\n",
       "  '201': 0.875,\n",
       "  '210': 0.75,\n",
       "  '211': 1.0,\n",
       "  '206': 0.875,\n",
       "  '103': 1.0,\n",
       "  '203': 0.5,\n",
       "  '305': 1.0,\n",
       "  '217': 1.0,\n",
       "  '105': 1.0,\n",
       "  '304': 1.0,\n",
       "  '302': 1.0,\n",
       "  '212': 0.8333333,\n",
       "  '102': 1.0,\n",
       "  '306': 1.0,\n",
       "  '214': 1.0,\n",
       "  '307': 1.0,\n",
       "  '216': 1.0,\n",
       "  '215': 0.8333333,\n",
       "  '207': 0.75,\n",
       "  '301': 1.0,\n",
       "  '101': 0.8333333},\n",
       " '4': {'104': 0.6666667,\n",
       "  '213': 1.0,\n",
       "  '303': 0.75,\n",
       "  '204': 0.5,\n",
       "  '201': 1.0,\n",
       "  '210': 0.75,\n",
       "  '211': 1.0,\n",
       "  '206': 0.41666666,\n",
       "  '103': 0.875,\n",
       "  '305': 0.8333333,\n",
       "  '202': 0.41666666,\n",
       "  '217': 1.0,\n",
       "  '105': 1.0,\n",
       "  '304': 0.8333333,\n",
       "  '302': 1.0,\n",
       "  '212': 0.6666667,\n",
       "  '102': 1.0,\n",
       "  '306': 1.0,\n",
       "  '214': 1.0,\n",
       "  '307': 1.0,\n",
       "  '216': 1.0,\n",
       "  '215': 0.6,\n",
       "  '301': 1.0,\n",
       "  '101': 0.8333333},\n",
       " '5': {'104': 1.0,\n",
       "  '209': 1.0,\n",
       "  '213': 0.5,\n",
       "  '303': 0.9166667,\n",
       "  '204': 0.6666667,\n",
       "  '201': 1.0,\n",
       "  '210': 0.6666667,\n",
       "  '211': 1.0,\n",
       "  '206': 0.5,\n",
       "  '103': 1.0,\n",
       "  '203': 0.6666667,\n",
       "  '305': 0.9166667,\n",
       "  '217': 1.0,\n",
       "  '105': 0.8333333,\n",
       "  '304': 1.0,\n",
       "  '302': 1.0,\n",
       "  '212': 0.6666667,\n",
       "  '102': 1.0,\n",
       "  '306': 1.0,\n",
       "  '214': 0.75,\n",
       "  '307': 1.0,\n",
       "  '216': 1.0,\n",
       "  '215': 0.875,\n",
       "  '301': 0.9166667,\n",
       "  '101': 1.0},\n",
       " '6': {'104': 1.0,\n",
       "  '209': 1.0,\n",
       "  '213': 1.0,\n",
       "  '303': 1.0,\n",
       "  '204': 0.8333333,\n",
       "  '201': 0.6666667,\n",
       "  '210': 0.0,\n",
       "  '211': 1.0,\n",
       "  '206': 0.85714287,\n",
       "  '103': 1.0,\n",
       "  '305': 1.0,\n",
       "  '217': 0.8333333,\n",
       "  '105': 0.75,\n",
       "  '304': 1.0,\n",
       "  '302': 1.0,\n",
       "  '212': 0.75,\n",
       "  '102': 1.0,\n",
       "  '306': 1.0,\n",
       "  '214': 0.5,\n",
       "  '307': 0.9166667,\n",
       "  '216': 0.8333333,\n",
       "  '215': 1.0,\n",
       "  '207': 0.95,\n",
       "  '301': 0.9166667,\n",
       "  '101': 1.0},\n",
       " '7': {'104': 0.8333333,\n",
       "  '209': 0.33333334,\n",
       "  '213': 1.0,\n",
       "  '303': 0.9166667,\n",
       "  '204': 1.0,\n",
       "  '211': 1.0,\n",
       "  '206': 0.75,\n",
       "  '103': 1.0,\n",
       "  '305': 1.0,\n",
       "  '202': 1.0,\n",
       "  '217': 1.0,\n",
       "  '105': 0.9166667,\n",
       "  '304': 0.8333333,\n",
       "  '302': 0.8333333,\n",
       "  '212': 0.5,\n",
       "  '102': 0.9583333,\n",
       "  '306': 0.9166667,\n",
       "  '214': 0.8333333,\n",
       "  '307': 1.0,\n",
       "  '216': 0.5,\n",
       "  '215': 1.0,\n",
       "  '207': 0.5,\n",
       "  '301': 1.0,\n",
       "  '101': 1.0},\n",
       " '8': {'104': 1.0,\n",
       "  '209': 0.75,\n",
       "  '213': 0.33333334,\n",
       "  '303': 0.9166667,\n",
       "  '204': 0.9166667,\n",
       "  '211': 1.0,\n",
       "  '206': 0.6,\n",
       "  '103': 0.9583333,\n",
       "  '305': 0.9,\n",
       "  '202': 0.8695652,\n",
       "  '217': 1.0,\n",
       "  '105': 1.0,\n",
       "  '304': 0.8333333,\n",
       "  '302': 1.0,\n",
       "  '212': 0.6666667,\n",
       "  '102': 0.9583333,\n",
       "  '306': 1.0,\n",
       "  '214': 0.33333334,\n",
       "  '307': 0.9166667,\n",
       "  '216': 1.0,\n",
       "  '215': 0.8,\n",
       "  '207': 0.16666667,\n",
       "  '301': 1.0,\n",
       "  '101': 1.0}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAESCAYAAADjS5I+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGEpJREFUeJzt3XmUZnV95/H3J7TIokgjrbJpN4oimkRID2Kcg8ZmoqjDEhFXgoqSjGhEnXFNJC4JggaXkThio7SgIAIKrhEUEz0OLQ1iAJFVgs1aOcimjoB+5497S8uy+lY93U/Vfbrq/TqnTlXde5/nfmm661P3t6aqkCRpXf6g7wIkSaPNoJAkdTIoJEmdDApJUieDQpLUyaCQJHUyKCRJnQwKSVIng0KS1GlR3wUMw7bbbltLly7tuwxJ2qhcdNFF/1lVS6a7bl4ExdKlS1mzZk3fZUjSRiXJf8zkOpueJEmdDApJUieDQpLUyaCQJHUyKCRJnQwKSVIng0KS1MmgkCR1MigkSZ3mxczsDXH8m06Z0/sdcexL5/R+G7u5/P/j/xtpaj5RSJI6GRSSpE4GhSSpk0EhSeq04DuzpY3NKA3AcLDBwuAThSSpk08U+j2j9BurpP4ZFJI2ev5yM7tsepIkdTIoJEmdDApJUieDQpLUyc5saQbsLNVC5hOFJKmTTxSSNETzcba6TxSSpE4GhSSpU69BkeT1SS5PclmSU5NslmRZktVJrk7y2SSb9lmjJC10vQVFkh2AvwGWV9UTgU2AFwLHAB+oql2AnwKH9VWjJKn/pqdFwOZJFgFbADcDzwDOaM+vAg7oqTZJEj0GRVXdCLwfuIEmIO4ELgLuqKr728vWAjv0U6EkCfpteloM7A8sA7YHtgT2neLSWsfrD0+yJsmasbGx2StUkha4Ppue9gF+XFVjVXUfcBbwp8DWbVMUwI7ATVO9uKpOqKrlVbV8yZIlc1OxJC1AfQbFDcBeSbZIEmAF8EPgfOCg9ppDgbN7qk+SRL99FKtpOq0vBi5tazkBeDPwhiTXAA8FTuyrRklSz0t4VNVRwFGTDl8H7NlDOZKkKbjW0wiZj2vESNr49T2PQpI04gwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCktTJoJAkdTIoJEmdDApJUieDQpLUyaCQJHUyKCRJnQwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCktRpvffMTrIl8FxgJ+Am4MtVdeewCpMkjYb1CookTwTOA7YGbgeWAHcm2beqLhxifZKknq1v09OHgC8Ci6tqe2BH4Ergfw+rMEnSaOgMiiRvTDLVNY8Hjq+qXwBU1a3AycBuwy9RktSn6Z4oXgd8L8mTJh2/HHh1kgcCJFkCHAL8cPglSpL6NF1Q7AasBi5IckySzdrjbwD2A+5IciNwI7Ar8NpZq1SS1IvOzuyqugc4IskpwAnAQUn+qqrOS/IYmrDYgd+Oerpj1iuWJM2pGY16qqr/m2QP4K3AF5N8Fnh9VX1mVquTJPVuxqOequq+qnoXsDuwM3BlkhfPWmWSpJEwbVAk2TLJiiT7Jdmhqn5UVXsDfwccn+SrSR41+6VKkvow3fDYJ9HMjzgX+AJwTZJXAVTVx2g6u38OXJbk9Ukyy/VKkubYdE8UH6bpqN4ZWAysBD6UZGuAqrq5qp5HMzT2DcD3ZrFWSVIPpguK3YGVVXV9u47T+4DNgMdOvKiqvgA8AYNCkuad6YJiLfC0Cd/vDRTNvInfUVV3VdURQ6xNkjQCphse+27g5CRPBu6gecJYVVW/FxSSpPlpugl3n0lyDXAAsDlwbFWdPieVSZJGwrQT7qrqe9j3IEkLVq873CXZOskZSX6U5IokT0myTZJzk1zdfl7cZ42StND1vRXqh4CvVdWuwB8DVwBvAb5RVbsA32i/lyT1pLegSLIVzSiqEwGq6t52UcH9gVXtZato+kckST3p84liZ2AM+GSS7ydZ2e7D/fCquhmaCX3Aw3qsUZIWvD6DYhGwB/DRqtod+BkDNDMlOTzJmiRrxsbGZqtGSVrw+gyKtcDaqlrdfn8GTXDcmmQ7gPbzbVO9uKpOqKrlVbV8yZIlc1KwJC1EMw6KJFcleXOSRwzjxlV1C/CTJI9rD62g2Ur1HODQ9tihwNnDuJ8kaf3MaOOi1n3A0cC7k3yFZoHAr1TVrzfg/q8FPp1kU+A64OU04XV6ksOAG4Dnb8D7S5I20IyDoqqekGQv4DDgYOC/A7ckOQn4RFVdO+jNq+oSYPkUp1YM+l6SpNkxUB9FVV1QVa8CtgNeCfyYZnvUq5J8M8mLkzxwFuqUJPVkvTqzq+rnVfXJqvqvwK7AacDTgZOBm5J8IMkjh1emJKkv6z3qKckmSQ4EjgNeQLP8+PnABTR9D1ck2X8oVUqSejNwUCTZNcn7aPakOJOmj+H9wGOrap+qeg7NU8aVwLHDLFaSNPdm3Jmd5BU0Hdl7tYfOA04Azq6q+ydeW1XXJPkwzcgoSdJGbJDhsSuBW4D3Ah+vquunuf6HNH0WkqSN2CBB8TzgnKr61Uwudh8LSZofBplH8fnZLESSNJoGWcLjnUku6zj/70n+djhlSZJGxSCjng4Ezu04fy5w0IaVI0kaNYMExTLgRx3nr2yvkSTNI4POo9i649xiYJMNqEWSNIIGCYrLabYp/T1JAuxH9xOHJGkjNEhQnAjsleSkJL/ZKaj9+hM0E/FOHHJ9kqSeDTI89uNJngb8JXBIkptp1nfaHgjw2ar66OyUKUnqy6DLjL8UeCHwJeBO4G6aHekOrqoXDb88SVLfBpmZDUBVnQ6cPgu1SJJG0HovMy5JWhgGfqJIshx4Ms1w2MlBU1X17mEUJkkaDYMsM745cBbw5zSd19V+ZsLXBRgUkjSPDNL09A6akPgH4M9oguFQYF/g28CFwG7DLlCS1K9BguIg4HNV9Q5gfHHAG6vqX4B9gE2Blw23PElS3wYJip2Af22/Ht+TYlOAdoe7U2mGzkqS5pFBguJuftuncTfwa5rJduPuBB4xpLokSSNikKC4FngsQLvL3eW0y4q3az39BfCTYRcoSerXIEFxHvC8JOMrxH4MeFaSa4GrafopXOtJkuaZQeZRvBc4mXZIbFX9c5LNgJfS9Fl8HDh26BVKkno1yKKA99BsTjTx2HHAccMuSpI0OmbU9JTkQUmuTXLkbBckSRotMwqK9mniocA9s1uOJGnUDNKZfQGwfLYKkSSNpkGC4i3AwUle3g6HlSQtAIOMejoO+CmwEji2HRb780nXVFWtGFZxkqT+DRIUO9OsDntD+/3Dh1+OJGnUDDI8duks1iFJGlHucCdJ6mRQSJI6DbLD3XUzuKyq6tEbUI8kacQM0pl9A01n9uTXL6NZbvwa4MZBC2gXGVxDswnSc5MsA04DtgEuBg6pqnsHfV9J0nAM0pn99HWdS/Ii4J+Av16PGl4HXAFs1X5/DPCBqjotyf8BDgM+uh7vK0kagqH0UVTVqcAXaMJixpLsCDyHZm7G+L4WzwDOaC9ZBRwwjBolSetnmJ3ZlwB7D/iaDwJvotktD5r1pO5ot1YFWAvsMNULkxyeZE2SNWNjY+tTryRpBoYZFE/itz/wp5XkucBtVXXRxMNTXDq5X6Q5WHVCVS2vquVLliwZrFJJ0owNMuppXU8L29Dsbvcq4KwB7v1UYL8kzwY2o+mj+CCwdZJF7VPFjsBNA7ynJGnIBhn19C2m/u1+/CngPOC1M32zqnor8FaAJE8H/mdVvSTJ52j24j4NOBQ4e4AaJUlDNkhQvHyKYwXcDlxVVVcNpyTeDJyW5D3A93Efbknq1SDDY1fNVhFV9S2aJxaq6jpgz9m6lyRpMDPuzE6yKMlWHee3SjLIE4okaSMwyKinf6KZQb0uF9JMlpMkzSODBMUzgTM7zp8J7Lth5UiSRs0gQbETcG3H+evaayRJ88ggQXEvsF3H+UcwwIQ7SdLGYZCg+D5wcJJNJ59oj70A+PdhFSZJGg2DBMXxwBOALydZnmTT9mM58CVgN+Ajs1GkJKk/g8yjODPJ0TSzqVfTTLYrmrAJcExVfXZWqpQk9WageQ9V9fYkXwBeCjyGJiCuBD5TVRfOQn2SpJ4NPEGuDQRDQZIWiEFmZm+T5I86zv9RksXDKUuSNCoG6cw+Fjip4/wngaM3qBpJ0sgZJCj+DPhix/lzaPalkCTNI4MExfbADR3n17bXSJLmkUGC4mfAozrOPwr45YaVI0kaNYMExWrg0CQPnnyiPfaXwPeGVZgkaTQMEhTvp9nD+rtJDkrymCSPTnIQ8N323Ptmo0hJUn8GmZl9fpJXAx8CJs/Avg94TVWdN8ziJEn9G3Rm9seSfAk4mN+dmX1GVd04C/VJknq2PjOzbwQ+MNW5JA+sKju0JWkeGaSPYp2S/EmSfwZuGsb7SZJGx8BPFOOSbEOzOOBhwBNpmqGuGlJdkqQRMfATRZJnJvksMN4EtSnwTuAPq2rXIdcnSerZjJ4okiwDXg4cSjMMdgw4A3gx8PaqOmvWKpQk9arziSLJi5N8A7gaeBOwBjgQ2IHmKSKzXqEkqVfTPVGcAlwHHEmzOdHt4yeS1GwWJkkaDdP1UdwLLAX2B/ZNsvmsVyRJGinTBcUjaJ4mHgqcDNya5MQke2OzkyQtCJ1BUVV3VNVHqmoPYDlNWBwAnA98ByjgIbNepSSpNzMeHltVF1fVETR7ThwCXN6eWpnkkiR/m+QJs1GkJKk/A8+jqKpfVtVnqmoF8GjgH4DFwLuAHwy5PklSzzZoCY+qur6q3kHT4f1swPkUkjTPrPcSHhNVVQFfaz8kSfPIUBYFlCTNXwaFJKmTQSFJ6mRQSJI69RYUSXZKcn6SK5JcnuR17fFtkpyb5Or28+K+apQk9ftEcT/wxqp6PLAXcESS3YC3AN+oql2Ab7TfS5J60ltQVNXNVXVx+/XdwBU0y5fvD6xqL1tFs2SIJKknI9FHkWQpsDuwGnh4Vd0MTZgAD+uvMklS70GR5EHAmcCRVXXXAK87PMmaJGvGxsZmr0BJWuB6DYokD6AJiU9P2E711iTbtee3A26b6rVVdUJVLa+q5UuWLJmbgiVpAepz1FOAE4Erquq4CafOodmbm/bz2XNdmyTpt4ay1tN6eirNcuWXJrmkPfY24L3A6UkOA24Ant9TfZIkegyKqvoO694lb8Vc1iJJWrfeO7MlSaPNoJAkdTIoJEmdDApJUieDQpLUyaCQJHUyKCRJnQwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCktTJoJAkdTIoJEmdDApJUieDQpLUyaCQJHUyKCRJnQwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCktTJoJAkdTIoJEmdDApJUieDQpLUyaCQJHUyKCRJnQwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCktRpJIMiybOSXJnkmiRv6bseSVrIRi4okmwCHA/sC+wGvCjJbv1WJUkL18gFBbAncE1VXVdV9wKnAfv3XJMkLVijGBQ7AD+Z8P3a9pgkqQepqr5r+B1Jng88s6pe2X5/CLBnVb120nWHA4e33z4OuHJOC4Vtgf+c43tOZVTqAGuZyqjUAaNTy6jUAdbyqKpaMt1Fi+aikgGtBXaa8P2OwE2TL6qqE4AT5qqoyZKsqarlfd1/1OoAaxnlOmB0ahmVOsBaZmoUm54uBHZJsizJpsALgXN6rkmSFqyRe6KoqvuTvAb4F2AT4BNVdXnPZUnSgjVyQQFQVV8BvtJ3HdPordlrklGpA6xlKqNSB4xOLaNSB1jLjIxcZ7YkabSMYh+FJGmEGBQDGpXlRZJ8IsltSS7rq4YJteyU5PwkVyS5PMnreqpjsyTfS/KDto539lHHpJo2SfL9JF/qsYbrk1ya5JIka/qqo61l6yRnJPlR+/flKT3V8bj2z2P8464kR/ZUy+vbv6+XJTk1yWZ91NHFpqcBtMuLXAX8N5phvBcCL6qqH/ZQy97APcCnquqJc33/SbVsB2xXVRcneTBwEXDAXP+5JAmwZVXdk+QBwHeA11XVBXNZx6Sa3gAsB7aqquf2VMP1wPKq6n2+QJJVwLeramU7qnGLqrqj55o2AW4EnlxV/zHH996B5u/pblX1iySnA1+pqpPmso7p+EQxmJFZXqSq/g24vY97T1ZVN1fVxe3XdwNX0MNs+mrc0377gPajt9+EkuwIPAdY2VcNoyTJVsDewIkAVXVv3yHRWgFcO9chMcEiYPMki4AtmGLeWN8MisG4vMg0kiwFdgdW93T/TZJcAtwGnFtVvdTR+iDwJuDXPdYATVh+PclF7YoGfdkZGAM+2TbHrUyyZY/1jHshcGofN66qG4H3AzcANwN3VtXX+6ili0ExmExxzLa7VpIHAWcCR1bVXX3UUFW/qqon0czo3zNJL81ySZ4L3FZVF/Vx/0meWlV70KzIfETbbNmHRcAewEeranfgZ0Cv2wi0zV/7AZ/r6f6LaVollgHbA1smeWkftXQxKAYzo+VFFqK2T+BM4NNVdVbf9bRNGt8CntVTCU8F9mv7B04DnpHklD4Kqaqb2s+3AZ+naULtw1pg7YSnvDNogqNP+wIXV9WtPd1/H+DHVTVWVfcBZwF/2lMt62RQDMblRabQdiKfCFxRVcf1WMeSJFu3X29O84/wR33UUlVvraodq2opzd+Tb1bVnP+mmGTLdoABbTPPnwO9jJSrqluAnyR5XHtoBTDnA0EmeRE9NTu1bgD2SrJF++9oBU0f30gZyZnZo2qUlhdJcirwdGDbJGuBo6rqxD5qofnt+RDg0rZ/AOBt7Qz7ubQdsKodxfIHwOlV1duw1BHxcODzzc8gFgGfqaqv9VjPa4FPt79oXQe8vK9CkmxBM4Lxr/qqoapWJzkDuBi4H/g+IzhD2+GxkqRONj1JkjoZFJKkTgaFJKmTQSFJ6mRQSJI6GRSSpE4Ghea1dunxVyf5ZpKxJPcluSPJhUmOSbLrHNTw90kOmO37SLPFeRSat5LsDHwJeDzwr8DXaRZeexDwJJo1frYBHtkuzjZbdRSwqqpeNlv3kGaTM7M1L7VLeHwZeDTwF1X1+Smu2Qx4PRvxwo7tf+d9VXV/37Vo/rLpSfPVK4FdgfdNFRIAVfX/quro8UXzxiV5SNssdU2SX7ZNVqe2TygTr9usbVa6MsnP2yatS5O8rz2/tH2aADg0SY1/TDyf5O8n19a+b7XLto8fO6k9tiTNDoe30qzAuuOEa16Q5DtJ7m5rWp3koMH/+KTf8olC89X4D8eBNg1K8hDgu8AjgU8Al9OsIfVqYHWS5RM2uDkeeAXwKeADNOt/7QI8oz0/RrMG1snAtxneGj7nArcA7wa2pNnpkCTvAd4OfA34O5p9MA4EPpfkNVV1/JDurwXGoNB89UTgrqr68cSD7YKBiydd+7Oq+kX79btoNtjZq6p+MOF1JwGXAu8EXtYePhD4alUdOlUBVfUz4JQkJwPXVdWwlhm/bPJKtEn2oAmJo6vqbRNOfTjJF4Cjk3yq3YFQGohNT5qvtgKm2jzp8TS/6U/8OAJ+s1z6S4B/A25Msu34B00TzwU0y3SPuxN4Qg+bI71/imMvoelrWTWx7rb2c4AHA0+ZyyI1f/hEofnqLpqwmOzHNEtLA/wxv/tDdwnwUJowGFvH+07c1vRImmalS5NcB5wPfBH4YlXN5vanV01x7PE0OzB27b/x8NkpR/OdQaH56jJg7yTLJjY/tc1B5wEkmTxSaHyr2/OAY6a7QVWd3XY2Pxt4Gs1GSYcB306yT1XdO91bdJxb57/Nqvr5FIfTvt++wK/W8dJe9k7Rxs+g0Hx1BrA3zeint8/wNWPAHcBWVXXeTF5QVbcDp9D0RQR4L/Ammn2Qp9uH+fb28zZTnNt5imNdrqbZ9vWGqhq5HdK0cbOPQvPVSppmmP+V5MB1XJOJ37TNRZ8G9lzXkNIkD2s/bzK+7eqE1xfNDmXwuz/872GKMGg7lm+h2U/7N7W0w3AHncl9cvv5H9sO+ynrltaHTxSal6rqF0meQzMz+6wk36KZmX0LTd/FrsALaJppfjLhpW+n2dr19CSn03Rg3ws8iqaJ6SKaUU8PBm5Ocg5NONwGLAP+B/BTmr6KcRcA+yR5M80eyVVVp7XnPgK8B/hqOzppe+CvaZrO/ssA/70XJjmKZlTWJUk+B9xEM7T3T9raN53p+0kTuYSH5rV25vIraOZV/CHwEJoRTNcA3wROrKorJ71mC+CNwMHAY2j2Ml4LfAdY2e5zvCnND+UVNLO/H0SzPMg3aYaoXj3h/XahmXOxF03AUFVpzy0C/pFmvsVi4IfAUTQ/3I8CllXV9e21JwGHjr92Hf+9zwH+hiZktqQJsMuAc6rqo4P82UnjDApJUif7KCRJnQwKSVIng0KS1MmgkCR1MigkSZ0MCklSJ4NCktTJoJAkdTIoJEmdDApJUqf/D/R1ujxFTl8uAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D = {}\n",
    "\n",
    "for k,v in overall_results.items():\n",
    "    vals = (list(v.values()))\n",
    "    avg = sum(vals) / len(vals)\n",
    "    D[k] = avg*100\n",
    "    \n",
    "# Create bars\n",
    "barWidth = 1\n",
    "bars1 = list(D.values())\n",
    "\n",
    "# The X position of bars\n",
    "r1 = range(len(D))\n",
    "\n",
    "plt.bar(r1, bars1, align='center', color = (0.3,0.1,0.4,0.6))\n",
    "\n",
    "plt.xticks(range(len(D)), list(D.keys()))\n",
    "\n",
    "\n",
    "plt.xlabel('Gesture',  fontsize=18)\n",
    "plt.ylabel('Accuracy %', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.62621985477209"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(list(D.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'104': 1.0,\n",
       " '209': 0.8333333,\n",
       " '205': 1.0,\n",
       " '213': 1.0,\n",
       " '303': 1.0,\n",
       " '204': 1.0,\n",
       " '201': 0.6666667,\n",
       " '211': 0.75,\n",
       " '206': 1.0,\n",
       " '103': 0.9583333,\n",
       " '305': 1.0,\n",
       " '217': 0.75,\n",
       " '105': 1.0,\n",
       " '304': 1.0,\n",
       " '302': 1.0,\n",
       " '212': 0.6666667,\n",
       " '102': 1.0,\n",
       " '306': 0.75,\n",
       " '214': 0.6666667,\n",
       " '307': 0.9,\n",
       " '216': 0.8333333,\n",
       " '215': 0.6666667,\n",
       " '207': 1.0,\n",
       " '301': 1.0,\n",
       " '101': 0.875}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_results['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "13\n",
      "0.6666667 1.0\n",
      "----\n",
      "1\n",
      "8\n",
      "0.0 1.0\n",
      "----\n",
      "2\n",
      "11\n",
      "0.0 1.0\n",
      "----\n",
      "3\n",
      "16\n",
      "0.16666667 1.0\n",
      "----\n",
      "4\n",
      "12\n",
      "0.41666666 1.0\n",
      "----\n",
      "5\n",
      "13\n",
      "0.5 1.0\n",
      "----\n",
      "6\n",
      "13\n",
      "0.0 1.0\n",
      "----\n",
      "7\n",
      "11\n",
      "0.33333334 1.0\n",
      "----\n",
      "8\n",
      "9\n",
      "0.16666667 1.0\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "moves = ['0','1','2','3','4','5','6','7','8']\n",
    "for m in moves:\n",
    "    print(m)\n",
    "    print(list(overall_results[m].values()).count(1))\n",
    "    print(min(overall_results[m].values()), max(overall_results[m].values()))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
